{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NYC Job Analysis Program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All Imports needed for Program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, udf, year, avg, when, max as spark_max, regexp_replace, explode, split, lower, trim, count,length, to_json, from_json,array_distinct,countDistinct, sum as Fsum\n",
    "from pyspark.sql.types import StructType, StructField,DoubleType, StringType, TimestampType, ArrayType, IntegerType, LongType, FloatType, DateType\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "import os\n",
    "import re\n",
    "import unittest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logging Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark Session Factory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SparkFactory:\n",
    "    \"\"\"\n",
    "    A factory class to create and configure a SparkSession for data processing.\n",
    "\n",
    "    This class provides a static method to initialize a SparkSession with\n",
    "    commonly used configurations for performance optimization.\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def create(app_name=\"NYC Jobs Analysis\"):\n",
    "        \"\"\"\n",
    "        Create and return a SparkSession with specified configurations.\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        app_name : str, optional\n",
    "            Name of the Spark application (default is \"NYC Jobs Analysis\").\n",
    "\n",
    "        Returns:\n",
    "        -------\n",
    "        pyspark.sql.SparkSession\n",
    "            A configured SparkSession instance.\n",
    "        \"\"\"\n",
    "        return (\n",
    "            SparkSession.builder\n",
    "            .appName(app_name)  # Set the name of the Spark application\n",
    "            .config(\"spark.sql.adaptive.enabled\", \"true\")  # Enable adaptive query execution\n",
    "            .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")  # Use Kryo for efficient serialization\n",
    "            .getOrCreate()  # Create or get existing SparkSession\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize logger for use in notebook (if not already set)\n",
    "logger = logging.getLogger(\"JobDataExtractor\")\n",
    "if not logger.handlers:\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "class JobDataExtractor:\n",
    "    \"\"\"\n",
    "    Extracts job data from a CSV file into a Spark DataFrame.\n",
    "    \n",
    "    Attributes:\n",
    "    ----------\n",
    "    spark : SparkSession\n",
    "        An active SparkSession object.\n",
    "    input_path : str\n",
    "        Path to the input CSV file.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, spark, input_path):\n",
    "        \"\"\"\n",
    "        Initialize the extractor with a SparkSession and file path.\n",
    "        \n",
    "        Parameters:\n",
    "        ----------\n",
    "        spark : SparkSession\n",
    "            The Spark session to use for reading data.\n",
    "        input_path : str\n",
    "            The path to the CSV input file.\n",
    "        \"\"\"\n",
    "        self.spark = spark\n",
    "        self.input_path = input_path\n",
    "\n",
    "    def read(self):\n",
    "        \"\"\"\n",
    "        Reads data from the CSV file into a Spark DataFrame.\n",
    "\n",
    "        Returns:\n",
    "        -------\n",
    "        pyspark.sql.DataFrame or None\n",
    "            Returns a DataFrame if data is present; otherwise, None.\n",
    "        \"\"\"\n",
    "        # Check if the input file is empty\n",
    "        if os.stat(self.input_path).st_size == 0:\n",
    "            logger.warning(\"Input file is empty.\")\n",
    "            return None\n",
    "\n",
    "        # Read CSV with common options for messy/complex data\n",
    "        df = (\n",
    "            self.spark.read\n",
    "            .option(\"header\", \"true\")\n",
    "            .option(\"inferSchema\", \"true\")\n",
    "            .option(\"multiLine\", \"true\")  # Supports multiline records\n",
    "            .option(\"quote\", \"\\\"\")        # Handle quoted fields\n",
    "            .option(\"escape\", \"\\\"\")       # Handle escape characters\n",
    "            .option(\"mode\", \"PERMISSIVE\") # Tolerate corrupt lines\n",
    "            .option(\"encoding\", \"ISO-8859-1\")  # Handle special encoding\n",
    "            .csv(self.input_path)\n",
    "        )\n",
    "\n",
    "        logger.info(\"Raw data loaded successfully.\")\n",
    "        return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Wrangling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JobDataWrangler:\n",
    "    \"\"\"\n",
    "    Cleans and standardizes column names in a Spark DataFrame.\n",
    "\n",
    "    Attributes:\n",
    "    ----------\n",
    "    df : pyspark.sql.DataFrame\n",
    "        The Spark DataFrame to be cleaned.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, df):\n",
    "        \"\"\"\n",
    "        Initialize with a DataFrame to wrangle.\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        df : pyspark.sql.DataFrame\n",
    "            The DataFrame whose column names need sanitization.\n",
    "        \"\"\"\n",
    "        self.df = df\n",
    "\n",
    "    @staticmethod\n",
    "    def sanitize_column_name(name):\n",
    "        \"\"\"\n",
    "        Convert a column name to camelCase by:\n",
    "        - Replacing non-alphanumeric characters with spaces\n",
    "        - Applying camelCase formatting\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        name : str\n",
    "            The original column name.\n",
    "\n",
    "        Returns:\n",
    "        -------\n",
    "        str\n",
    "            The sanitized column name in camelCase.\n",
    "        \"\"\"\n",
    "        name = re.sub(r'[^\\w]', ' ', name)  # Replace non-word characters with space\n",
    "        parts = name.strip().split()        # Split into words\n",
    "        # Convert to camelCase (e.g., \"Job Title\" -> \"jobTitle\")\n",
    "        return ''.join([parts[0].lower()] + [p.capitalize() for p in parts[1:]]) if parts else name\n",
    "\n",
    "    def sanitize_column_names(self):\n",
    "        \"\"\"\n",
    "        Sanitize all column names in the DataFrame using camelCase format.\n",
    "\n",
    "        Returns:\n",
    "        -------\n",
    "        pyspark.sql.DataFrame\n",
    "            The DataFrame with sanitized column names.\n",
    "        \"\"\"\n",
    "        for old_name in self.df.columns:\n",
    "            new_name = self.sanitize_column_name(old_name)\n",
    "            # Rename only if the name changes to avoid unnecessary transformations\n",
    "            if old_name != new_name:\n",
    "                self.df = self.df.withColumnRenamed(old_name, new_name)\n",
    "        return self.df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JobDataTransformer:\n",
    "    \"\"\"\n",
    "    Transforms raw job data by engineering new features like:\n",
    "    - Average salary\n",
    "    - Posting year\n",
    "    - Classified degree levels\n",
    "    - Parsed skills list\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, df):\n",
    "        \"\"\"\n",
    "        Initialize with a Spark DataFrame to be transformed.\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        df : pyspark.sql.DataFrame\n",
    "            The raw job data DataFrame.\n",
    "        \"\"\"\n",
    "        self.df = df\n",
    "\n",
    "    @staticmethod\n",
    "    def classify_degree(text):\n",
    "        \"\"\"\n",
    "        Classifies education level from text into standard categories.\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        text : str\n",
    "            The minimum qualification requirement text.\n",
    "\n",
    "        Returns:\n",
    "        -------\n",
    "        str\n",
    "            One of [\"PhD\", \"Masters\", \"Bachelors\", \"Associate\", \"High School\", \"Other\"]\n",
    "        \"\"\"\n",
    "        if not text:\n",
    "            return \"Other\"\n",
    "\n",
    "        text = text.lower()\n",
    "        patterns = [\n",
    "            (r\"(ph\\.?d|doctorate|doctoral)\", \"PhD\"),\n",
    "            (r\"(master'?s|m\\.?a\\.?|m\\.?s\\.?|mba)\", \"Masters\"),\n",
    "            (r\"(bachelor'?s|b\\.?a\\.?|b\\.?s\\.?|baccalaureate)\", \"Bachelors\"),\n",
    "            (r\"(associate'?s|a\\.?a\\.?|a\\.?s\\.?)\", \"Associate\"),\n",
    "            (r\"(high school|h\\.?s\\.?|diploma|ged)\", \"High School\"),\n",
    "        ]\n",
    "\n",
    "        for pattern, label in patterns:\n",
    "            if re.search(pattern, text):\n",
    "                return label\n",
    "\n",
    "        return \"Other\"\n",
    "\n",
    "    @staticmethod\n",
    "    def extract_skills(text):\n",
    "        \"\"\"\n",
    "        Extracts a list of possible skills from unstructured text.\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        text : str\n",
    "            The preferred skills text.\n",
    "\n",
    "        Returns:\n",
    "        -------\n",
    "        list[str]\n",
    "            A list of lowercase skill phrases.\n",
    "        \"\"\"\n",
    "        if not text:\n",
    "            return []\n",
    "\n",
    "        # Normalize text and remove special characters\n",
    "        text = re.sub(r'[^\\x00-\\x7F]+', ' ', text)\n",
    "        text = re.sub(r'[\\u2022\\u2023\\u25E6\\u2043\\u2219]', ' ', text)\n",
    "        text = text.replace('\\t', ' ').replace('\\n', ' ')\n",
    "\n",
    "        # Split on sentence-like punctuation and bullets\n",
    "        sentence_split = re.split(r'(?<=\\d\\.)|[.;\\n\\tâ€¢]|(?<!\\d)\\.\\s+', text)\n",
    "        skills = set()\n",
    "\n",
    "        for sentence in sentence_split:\n",
    "            sentence = sentence.strip().lower()\n",
    "            # Further split into potential skill phrases\n",
    "            clauses = re.split(r'[,-]', sentence)\n",
    "            for clause in clauses:\n",
    "                token = clause.strip()\n",
    "                if len(token) > 2:\n",
    "                    skills.add(token)\n",
    "\n",
    "        return list(skills)\n",
    "\n",
    "    def transform(self):\n",
    "        \"\"\"\n",
    "        Applies transformations:\n",
    "        - Computes average salary\n",
    "        - Extracts posting year\n",
    "        - Classifies degree level\n",
    "        - Parses skills into JSON string\n",
    "\n",
    "        Returns:\n",
    "        -------\n",
    "        pyspark.sql.DataFrame\n",
    "            Transformed DataFrame with derived features.\n",
    "        \"\"\"\n",
    "        df = self.df\n",
    "\n",
    "        # Register UDFs\n",
    "        degree_udf = udf(self.classify_degree, StringType())\n",
    "        skills_udf = udf(self.extract_skills, ArrayType(StringType()))\n",
    "\n",
    "        # Derived columns\n",
    "        df = df.withColumn(\"avgSalary\", (col(\"salaryRangeFrom\") + col(\"salaryRangeTo\")) / 2)\n",
    "        df = df.withColumn(\"postingYear\", year(col(\"postingDate\")))\n",
    "        df = df.withColumn(\"degreeLevel\", degree_udf(col(\"minimumQualRequirements\")))\n",
    "        df = df.withColumn(\"skillsJson\", to_json(skills_udf(col(\"preferredSkills\"))))\n",
    "\n",
    "        # Drop records with missing average salary\n",
    "        df = df.dropna(subset=[\"avgSalary\"])\n",
    "\n",
    "        # Select relevant columns only (if they exist)\n",
    "        selected_columns = [\n",
    "            \"agency\", \"businessTitle\", \"civilServiceTitle\", \"jobCategory\", \"postingDate\",\n",
    "            \"salaryRangeFrom\", \"salaryRangeTo\", \"avgSalary\", \"postingYear\", \"degreeLevel\", \"skillsJson\"\n",
    "        ]\n",
    "        df = df.select(*[c for c in selected_columns if c in df.columns])\n",
    "\n",
    "        df.cache()\n",
    "        logger.info(\"Data transformed successfully.\")\n",
    "        return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JobDataValidator:\n",
    "    \"\"\"\n",
    "    Validates cleaned job data by ensuring key fields are not null.\n",
    "\n",
    "    Attributes:\n",
    "    ----------\n",
    "    df : pyspark.sql.DataFrame\n",
    "        The Spark DataFrame to validate.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, df):\n",
    "        \"\"\"\n",
    "        Initialize with a DataFrame to validate.\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        df : pyspark.sql.DataFrame\n",
    "            The DataFrame to run validation checks on.\n",
    "        \"\"\"\n",
    "        self.df = df\n",
    "\n",
    "    def validate(self):\n",
    "        \"\"\"\n",
    "        Filters the DataFrame to retain only rows with required non-null fields.\n",
    "\n",
    "        Returns:\n",
    "        -------\n",
    "        pyspark.sql.DataFrame\n",
    "            A validated DataFrame with non-null 'avgSalary' and 'agency'.\n",
    "        \"\"\"\n",
    "        return (\n",
    "            self.df\n",
    "            .filter(col(\"avgSalary\").isNotNull())  # Ensure salary is present\n",
    "            .filter(col(\"agency\").isNotNull())     # Ensure agency is present\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JobDataLoader:\n",
    "    \"\"\"\n",
    "    Handles writing the transformed and validated job data to disk.\n",
    "\n",
    "    Attributes:\n",
    "    ----------\n",
    "    df : pyspark.sql.DataFrame\n",
    "        The Spark DataFrame to be saved.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, df):\n",
    "        \"\"\"\n",
    "        Initialize with the DataFrame to be written.\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        df : pyspark.sql.DataFrame\n",
    "            The DataFrame to output.\n",
    "        \"\"\"\n",
    "        self.df = df\n",
    "\n",
    "    def write(self, path):\n",
    "        \"\"\"\n",
    "        Write the DataFrame to the specified path as a CSV file.\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        path : str\n",
    "            Output directory path for the CSV file.\n",
    "        \"\"\"\n",
    "        # Coalesce to 1 partition to ensure single output file (suitable for smaller datasets)\n",
    "        self.df.coalesce(1) \\\n",
    "               .write \\\n",
    "               .mode(\"overwrite\") \\\n",
    "               .option(\"header\", \"true\") \\\n",
    "               .csv(path)\n",
    "\n",
    "        logger.info(f\"Refined data written to {path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JobDataProfiler:\n",
    "    \"\"\"\n",
    "    Profiles a Spark DataFrame for:\n",
    "    - Schema printing\n",
    "    - Null value analysis\n",
    "    - Unique value counts\n",
    "    - Data type classification\n",
    "\n",
    "    Attributes:\n",
    "    ----------\n",
    "    df : pyspark.sql.DataFrame\n",
    "        The DataFrame to profile.\n",
    "    numerical_cols : list\n",
    "        List of columns inferred as numerical.\n",
    "    categorical_cols : list\n",
    "        List of columns inferred as categorical.\n",
    "    date_cols : list\n",
    "        List of columns inferred as dates.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, df):\n",
    "        \"\"\"\n",
    "        Initialize the profiler with a Spark DataFrame.\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        df : pyspark.sql.DataFrame\n",
    "            The DataFrame to analyze.\n",
    "        \"\"\"\n",
    "        self.df = df\n",
    "        self.numerical_cols = []\n",
    "        self.categorical_cols = []\n",
    "        self.date_cols = []\n",
    "\n",
    "    def print_schema(self):\n",
    "        \"\"\"\n",
    "        Prints the schema of the DataFrame.\n",
    "        \"\"\"\n",
    "        print(\"Schema:\")\n",
    "        self.df.printSchema()\n",
    "\n",
    "    def count_nulls(self):\n",
    "        \"\"\"\n",
    "        Prints the count of null values for each column.\n",
    "        \"\"\"\n",
    "        print(\"Null Counts:\")\n",
    "        null_counts = self.df.select([\n",
    "            Fsum(col(c).isNull().cast(\"int\")).alias(c) for c in self.df.columns\n",
    "        ])\n",
    "        null_counts.show(truncate=False)\n",
    "\n",
    "    def count_unique(self):\n",
    "        \"\"\"\n",
    "        Prints the count of unique values for each column.\n",
    "        \"\"\"\n",
    "        print(\"Unique Value Counts:\")\n",
    "        for c in self.df.columns:\n",
    "            distinct_count = self.df.select(countDistinct(col(c))).collect()[0][0]\n",
    "            print(f\"{c}: {distinct_count} unique values\")\n",
    "\n",
    "    def detect_column_types(self):\n",
    "        \"\"\"\n",
    "        Classifies columns into numerical, categorical, and date types.\n",
    "\n",
    "        Outputs:\n",
    "        -------\n",
    "        Lists of column names by type are printed.\n",
    "        \"\"\"\n",
    "        print(\"Column Type Classification:\")\n",
    "        for field in self.df.schema.fields:\n",
    "            if isinstance(field.dataType, (DoubleType, IntegerType, LongType, FloatType)):\n",
    "                self.numerical_cols.append(field.name)\n",
    "            elif isinstance(field.dataType, (DateType, TimestampType)):\n",
    "                self.date_cols.append(field.name)\n",
    "            else:\n",
    "                self.categorical_cols.append(field.name)\n",
    "\n",
    "        print(f\"Categorical Columns: {self.categorical_cols}\")\n",
    "        print(f\"Numerical Columns: {self.numerical_cols}\")\n",
    "        print(f\"Date Columns: {self.date_cols}\")\n",
    "\n",
    "    def run_all(self):\n",
    "        \"\"\"\n",
    "        Runs all profiling steps in sequence.\n",
    "        \"\"\"\n",
    "        self.print_schema()\n",
    "        self.count_nulls()\n",
    "        self.count_unique()\n",
    "        self.detect_column_types()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KPI Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JobKPI:\n",
    "    \"\"\"\n",
    "    Computes key performance indicators (KPIs) from a transformed Spark DataFrame\n",
    "    containing job listings and derived fields like avgSalary, degreeLevel, skillsJson, etc.\n",
    "    \n",
    "    Attributes:\n",
    "    ----------\n",
    "    df : pyspark.sql.DataFrame\n",
    "        The Spark DataFrame on which KPI calculations will be run.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, df):\n",
    "        \"\"\"\n",
    "        Initialize with a Spark DataFrame.\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        df : pyspark.sql.DataFrame\n",
    "            The DataFrame to compute KPIs from.\n",
    "        \"\"\"\n",
    "        self.df = df\n",
    "\n",
    "    def jobs_per_category(self):\n",
    "        \"\"\"\n",
    "        Returns the top 10 job categories by job posting count.\n",
    "\n",
    "        Returns:\n",
    "        -------\n",
    "        pyspark.sql.DataFrame\n",
    "        \"\"\"\n",
    "        return (\n",
    "            self.df.groupBy(\"jobCategory\")\n",
    "            .count()\n",
    "            .orderBy(col(\"count\").desc())\n",
    "            .limit(10)\n",
    "        )\n",
    "\n",
    "    def salary_distribution(self):\n",
    "        \"\"\"\n",
    "        Computes average salary per job category.\n",
    "\n",
    "        Returns:\n",
    "        -------\n",
    "        pyspark.sql.DataFrame\n",
    "        \"\"\"\n",
    "        return (\n",
    "            self.df.groupBy(\"jobCategory\")\n",
    "            .agg(avg(\"avgSalary\").alias(\"avgSalary\"))\n",
    "            .orderBy(col(\"avgSalary\").desc())\n",
    "        )\n",
    "\n",
    "    def degree_vs_salary(self):\n",
    "        \"\"\"\n",
    "        Computes average salary by required degree level.\n",
    "\n",
    "        Returns:\n",
    "        -------\n",
    "        pyspark.sql.DataFrame\n",
    "        \"\"\"\n",
    "        return (\n",
    "            self.df.groupBy(\"degreeLevel\")\n",
    "            .agg(avg(\"avgSalary\").alias(\"avgSalary\"))\n",
    "            .orderBy(col(\"avgSalary\").desc())\n",
    "        )\n",
    "\n",
    "    def highest_salary_per_agency(self):\n",
    "        \"\"\"\n",
    "        Finds the highest average salary offered per agency.\n",
    "\n",
    "        Returns:\n",
    "        -------\n",
    "        pyspark.sql.DataFrame\n",
    "        \"\"\"\n",
    "        return (\n",
    "            self.df.withColumn(\"avgSalary\", col(\"avgSalary\").cast(DoubleType()))\n",
    "            .groupBy(\"agency\")\n",
    "            .agg(spark_max(\"avgSalary\").alias(\"maxSalary\"))\n",
    "            .orderBy(col(\"maxSalary\").desc())\n",
    "        )\n",
    "\n",
    "    def avg_salary_last_2_years(self):\n",
    "        \"\"\"\n",
    "        Calculates the average salary per agency over the last 2 posting years.\n",
    "\n",
    "        Returns:\n",
    "        -------\n",
    "        pyspark.sql.DataFrame\n",
    "        \"\"\"\n",
    "        # Find latest year in the dataset\n",
    "        latest_year = self.df.select(spark_max(\"postingYear\").alias(\"latest_year\")).collect()[0][\"latest_year\"]\n",
    "\n",
    "        return (\n",
    "            self.df.filter(col(\"postingYear\") >= (latest_year - 1))\n",
    "            .groupBy(\"agency\")\n",
    "            .agg(avg(\"avgSalary\").alias(\"avgSalary\"))\n",
    "            .orderBy(col(\"avgSalary\").desc())\n",
    "        )\n",
    "\n",
    "    def highest_paid_skills(self):\n",
    "        \"\"\"\n",
    "        Identifies top 10 skills with highest average salary.\n",
    "\n",
    "        Returns:\n",
    "        -------\n",
    "        pyspark.sql.DataFrame\n",
    "        \"\"\"\n",
    "        # Parse the skills JSON array and normalize\n",
    "        df_skills = (\n",
    "            self.df.withColumn(\"skillsArray\", from_json(col(\"skillsJson\"), ArrayType(StringType())))\n",
    "            .filter(col(\"skillsArray\").isNotNull())\n",
    "            .withColumn(\"skill\", explode(col(\"skillsArray\")))\n",
    "            .withColumn(\"skill\", trim(lower(col(\"skill\"))))\n",
    "            .filter((col(\"skill\") != \"\") & (length(col(\"skill\")) > 2))\n",
    "        )\n",
    "\n",
    "        # Compute average salary and usage count per skill\n",
    "        top_skills = (\n",
    "            df_skills.groupBy(\"skill\")\n",
    "            .agg(\n",
    "                avg(\"avgSalary\").alias(\"avgSalary\"),\n",
    "                count(\"skill\").alias(\"skillCount\")\n",
    "            )\n",
    "            .orderBy(col(\"avgSalary\").desc())\n",
    "            .limit(10)\n",
    "        )\n",
    "\n",
    "        return top_skills\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JobVisualizer:\n",
    "    \"\"\"\n",
    "    Generates bar chart visualizations from Spark DataFrames using Matplotlib.\n",
    "\n",
    "    Attributes:\n",
    "    ----------\n",
    "    output_dir : str\n",
    "        Directory where the charts will be saved.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, output_dir=\"/dataset/output/charts/\"):\n",
    "        \"\"\"\n",
    "        Initializes the visualizer and ensures the output directory exists.\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        output_dir : str, optional\n",
    "            Directory path for saving plots. Defaults to '/dataset/output/charts/'.\n",
    "        \"\"\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        self.output_dir = output_dir\n",
    "\n",
    "    def plot_bar(self, df, x_col, y_col, title, filename):\n",
    "        \"\"\"\n",
    "        Generates a horizontal bar plot from a Spark DataFrame.\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        df : pyspark.sql.DataFrame\n",
    "            The input DataFrame to visualize.\n",
    "        x_col : str\n",
    "            Column name for the y-axis (categories).\n",
    "        y_col : str\n",
    "            Column name for the x-axis (numeric values).\n",
    "        title : str\n",
    "            Title of the chart.\n",
    "        filename : str\n",
    "            Output file name for saving the chart (PNG format).\n",
    "        \"\"\"\n",
    "\n",
    "        # Convert Spark DataFrame to Pandas for visualization\n",
    "        pd_df = df.toPandas()\n",
    "\n",
    "        # Ensure y_col is numeric and drop missing values\n",
    "        pd_df[y_col] = pd.to_numeric(pd_df[y_col], errors='coerce')\n",
    "        pd_df = pd_df.dropna(subset=[y_col])\n",
    "\n",
    "        if pd_df.empty:\n",
    "            logger.warning(f\"No data to plot for {title}\")\n",
    "            return\n",
    "\n",
    "        # Dynamic sizing\n",
    "        num_bars = len(pd_df[x_col].unique())\n",
    "        fig_height = max(6, num_bars * 0.5)\n",
    "        fig_width = 20\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(fig_width, fig_height))\n",
    "\n",
    "        # Create horizontal bar chart\n",
    "        bars = pd_df.plot.barh(x=x_col, y=y_col, ax=ax, legend=False, color='skyblue')\n",
    "\n",
    "        ax.set_title(title, fontsize=16)\n",
    "        ax.tick_params(axis='y', labelsize=12)\n",
    "\n",
    "        # Adjust plot layout for visibility\n",
    "        plt.subplots_adjust(left=0.3, right=0.95)\n",
    "\n",
    "        # X-axis limit and labels\n",
    "        xmax = pd_df[y_col].max()\n",
    "        ax.set_xlim(0, xmax * 1.1)\n",
    "\n",
    "        # Annotate bars with values\n",
    "        for i, v in enumerate(pd_df[y_col]):\n",
    "            ax.text(v + xmax * 0.01, i, f\"{v:.2f}\", color='black', va='center', fontsize=10)\n",
    "\n",
    "        # Save chart to file\n",
    "        path = os.path.join(self.output_dir, filename)\n",
    "        fig.savefig(path, bbox_inches='tight')\n",
    "        plt.close(fig)\n",
    "\n",
    "        logger.info(f\"Chart saved: {path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Raw Data Extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:JobDataExtractor:Raw data loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "input_path = \"/dataset/nyc-jobs.csv\"\n",
    "refined_path =  \"/dataset/output/refineddata\"\n",
    "\n",
    "spark = SparkFactory.create()\n",
    "extractor = JobDataExtractor(spark, input_path)\n",
    "raw_df = extractor.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema:\n",
      "root\n",
      " |-- Job ID: integer (nullable = true)\n",
      " |-- Agency: string (nullable = true)\n",
      " |-- Posting Type: string (nullable = true)\n",
      " |-- # Of Positions: integer (nullable = true)\n",
      " |-- Business Title: string (nullable = true)\n",
      " |-- Civil Service Title: string (nullable = true)\n",
      " |-- Title Code No: string (nullable = true)\n",
      " |-- Level: string (nullable = true)\n",
      " |-- Job Category: string (nullable = true)\n",
      " |-- Full-Time/Part-Time indicator: string (nullable = true)\n",
      " |-- Salary Range From: double (nullable = true)\n",
      " |-- Salary Range To: double (nullable = true)\n",
      " |-- Salary Frequency: string (nullable = true)\n",
      " |-- Work Location: string (nullable = true)\n",
      " |-- Division/Work Unit: string (nullable = true)\n",
      " |-- Job Description: string (nullable = true)\n",
      " |-- Minimum Qual Requirements: string (nullable = true)\n",
      " |-- Preferred Skills: string (nullable = true)\n",
      " |-- Additional Information: string (nullable = true)\n",
      " |-- To Apply: string (nullable = true)\n",
      " |-- Hours/Shift: string (nullable = true)\n",
      " |-- Work Location 1: string (nullable = true)\n",
      " |-- Recruitment Contact: string (nullable = true)\n",
      " |-- Residency Requirement: string (nullable = true)\n",
      " |-- Posting Date: timestamp (nullable = true)\n",
      " |-- Post Until: timestamp (nullable = true)\n",
      " |-- Posting Updated: timestamp (nullable = true)\n",
      ": string (nullable = true)\n",
      "\n",
      "Null Counts:\n",
      "+------+------+------------+--------------+--------------+-------------------+-------------+-----+------------+-----------------------------+-----------------+---------------+----------------+-------------+------------------+---------------+-------------------------+----------------+----------------------+--------+-----------+---------------+-------------------+---------------------+------------+----------+---------------+-------------+\n",
      "|Job ID|Agency|Posting Type|# Of Positions|Business Title|Civil Service Title|Title Code No|Level|Job Category|Full-Time/Part-Time indicator|Salary Range From|Salary Range To|Salary Frequency|Work Location|Division/Work Unit|Job Description|Minimum Qual Requirements|Preferred Skills|Additional Information|To Apply|Hours/Shift|Work Location 1|Recruitment Contact|Residency Requirement|Posting Date|Post Until|Posting Updated|Process Date\n",
      "+------+------+------------+--------------+--------------+-------------------+-------------+-----+------------+-----------------------------+-----------------+---------------+----------------+-------------+------------------+---------------+-------------------------+----------------+----------------------+--------+-----------+---------------+-------------------+---------------------+------------+----------+---------------+-------------+\n",
      "|0     |0     |0           |0             |0             |0                  |0            |0    |2           |195                          |0                |0              |0               |0            |0                 |0              |20                       |393             |1092                  |1       |2062       |1588           |2946               |4                    |4           |2075      |4              |0            |\n",
      "+------+------+------------+--------------+--------------+-------------------+-------------+-----+------------+-----------------------------+-----------------+---------------+----------------+-------------+------------------+---------------+-------------------------+----------------+----------------------+--------+-----------+---------------+-------------------+---------------------+------------+----------+---------------+-------------+\n",
      "\n",
      "Unique Value Counts:\n",
      "Job ID: 1661 unique values\n",
      "Agency: 52 unique values\n",
      "Posting Type: 2 unique values\n",
      "# Of Positions: 34 unique values\n",
      "Business Title: 1244 unique values\n",
      "Civil Service Title: 312 unique values\n",
      "Title Code No: 323 unique values\n",
      "Level: 14 unique values\n",
      "Job Category: 130 unique values\n",
      "Full-Time/Part-Time indicator: 2 unique values\n",
      "Salary Range From: 519 unique values\n",
      "Salary Range To: 658 unique values\n",
      "Salary Frequency: 3 unique values\n",
      "Work Location: 226 unique values\n",
      "Division/Work Unit: 678 unique values\n",
      "Job Description: 1608 unique values\n",
      "Minimum Qual Requirements: 336 unique values\n",
      "Preferred Skills: 1282 unique values\n",
      "Additional Information: 681 unique values\n",
      "To Apply: 893 unique values\n",
      "Hours/Shift: 181 unique values\n",
      "Work Location 1: 227 unique values\n",
      "Recruitment Contact: 0 unique values\n",
      "Residency Requirement: 50 unique values\n",
      "Posting Date: 493 unique values\n",
      "Post Until: 105 unique values\n",
      "Posting Updated: 487 unique values\n",
      ": 2 unique values\n",
      "Column Type Classification:\n",
      "Categorical Columns: ['Agency', 'Posting Type', 'Business Title', 'Civil Service Title', 'Title Code No', 'Level', 'Job Category', 'Full-Time/Part-Time indicator', 'Salary Frequency', 'Work Location', 'Division/Work Unit', 'Job Description', 'Minimum Qual Requirements', 'Preferred Skills', 'Additional Information', 'To Apply', 'Hours/Shift', 'Work Location 1', 'Recruitment Contact', 'Residency Requirement', 'Process Date\\r']\n",
      "Numerical Columns: ['Job ID', '# Of Positions', 'Salary Range From', 'Salary Range To']\n",
      "Date Columns: ['Posting Date', 'Post Until', 'Posting Updated']\n"
     ]
    }
   ],
   "source": [
    "if raw_df is not None:\n",
    "    profiler = JobDataProfiler(raw_df)\n",
    "    profiler.run_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Wrangling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "    wrangler = JobDataWrangler(raw_df)\n",
    "    df_cleaned = wrangler.sanitize_column_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calling Data Enrichment Utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:JobDataExtractor:Data transformed successfully.\n"
     ]
    }
   ],
   "source": [
    "    transformer = JobDataTransformer(df_cleaned)\n",
    "    df_transformed = transformer.transform()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calling Data Validator Utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "    validator = JobDataValidator(df_transformed)\n",
    "    validated_df = validator.validate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calling Data Loader Utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:JobDataExtractor:Refined data written to /dataset/output/refineddata\n"
     ]
    }
   ],
   "source": [
    "    loader = JobDataLoader(validated_df)\n",
    "    loader.write(refined_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calling Data Visualizer Utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "    kpi = JobKPI(validated_df)\n",
    "    visualizer = JobVisualizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate charts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:JobDataExtractor:Chart saved: /dataset/output/charts/jobs_per_category.png\n"
     ]
    }
   ],
   "source": [
    "    visualizer.plot_bar(kpi.jobs_per_category(), \"jobCategory\", \"count\", \"Top 10 Job Categories\", \"jobs_per_category.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:JobDataExtractor:Chart saved: /dataset/output/charts/salary_distribution.png\n"
     ]
    }
   ],
   "source": [
    "    visualizer.plot_bar(kpi.salary_distribution(), \"jobCategory\", \"avgSalary\", \"Avg Salary by Job Category\", \"salary_distribution.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:JobDataExtractor:Chart saved: /dataset/output/charts/degree_vs_salary.png\n"
     ]
    }
   ],
   "source": [
    "    visualizer.plot_bar(kpi.degree_vs_salary(), \"degreeLevel\", \"avgSalary\", \"Avg Salary by Degree Level\", \"degree_vs_salary.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:JobDataExtractor:Chart saved: /dataset/output/charts/max_salary.png\n"
     ]
    }
   ],
   "source": [
    "    visualizer.plot_bar(kpi.highest_salary_per_agency(), \"agency\", \"maxSalary\", \"Max Salary per Agency\", \"max_salary.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:JobDataExtractor:Chart saved: /dataset/output/charts/avg_salary_2y.png\n"
     ]
    }
   ],
   "source": [
    "    visualizer.plot_bar(kpi.avg_salary_last_2_years(), \"agency\", \"avgSalary\", \"Avg Salary (Last 2 Years)\", \"avg_salary_2y.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:JobDataExtractor:Chart saved: /dataset/output/charts/highest_paid_skills.png\n"
     ]
    }
   ],
   "source": [
    "    visualizer.plot_bar(kpi.highest_paid_skills(), \"skill\", \"avgSalary\", \"Top 20 Highest Paid Skills\", \"highest_paid_skills.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unit Test Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dummy DataFrame\n",
    "data = [(\"Agency A\", \"Software Engineer\", \"Civil Title\", \"Tech\", \"2023-06-01\", 60000, 80000, \"Bachelor's degree required\", \"Python; SQL\")]\n",
    "schema = StructType([\n",
    "    StructField(\"agency\", StringType(), True),\n",
    "    StructField(\"businessTitle\", StringType(), True),\n",
    "    StructField(\"civilServiceTitle\", StringType(), True),\n",
    "    StructField(\"jobCategory\", StringType(), True),\n",
    "    StructField(\"postingDate\", StringType(), True),\n",
    "    StructField(\"salaryRangeFrom\", IntegerType(), True),\n",
    "    StructField(\"salaryRangeTo\", IntegerType(), True),\n",
    "    StructField(\"minimumQualRequirements\", StringType(), True),\n",
    "    StructField(\"preferredSkills\", StringType(), True)\n",
    "])\n",
    "df = spark.createDataFrame(data, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TestJobPipelineComponents(unittest.TestCase):\n",
    "\n",
    "    def test_spark_factory(self):\n",
    "        session = SparkFactory.create(\"TestApp\")\n",
    "        self.assertIsInstance(session, SparkSession)\n",
    "\n",
    "    def test_job_data_extractor(self):\n",
    "        path = \"/dataset/nyc-jobs.csv\"\n",
    "        df.toPandas().to_csv(path, index=False)\n",
    "        extractor = JobDataExtractor(spark, path)\n",
    "        result = extractor.read()\n",
    "        self.assertIsNotNone(result)\n",
    "        self.assertGreater(len(result.columns), 0)\n",
    "\n",
    "    def test_job_data_wrangler(self):\n",
    "        wrangler = JobDataWrangler(df)\n",
    "        result = wrangler.sanitize_column_names()\n",
    "        for col_name in result.columns:\n",
    "            self.assertFalse(re.search(r\"[^\\w]\", col_name))\n",
    "\n",
    "    def test_job_data_transformer(self):\n",
    "        transformer = JobDataTransformer(df)\n",
    "        result = transformer.transform()\n",
    "        self.assertIn(\"avgSalary\", result.columns)\n",
    "        self.assertIn(\"degreeLevel\", result.columns)\n",
    "        self.assertIn(\"skillsJson\", result.columns)\n",
    "\n",
    "    def test_job_data_validator(self):\n",
    "        transformer = JobDataTransformer(df)\n",
    "        transformed = transformer.transform()\n",
    "        validator = JobDataValidator(transformed)\n",
    "        validated = validator.validate()\n",
    "        self.assertTrue(validated.count() > 0)\n",
    "\n",
    "    def test_job_data_loader(self):\n",
    "        transformer = JobDataTransformer(df)\n",
    "        transformed = transformer.transform()\n",
    "        loader = JobDataLoader(transformed)\n",
    "        output_path = \"/dataset/output/testdata\"\n",
    "        loader.write(output_path)\n",
    "        self.assertTrue(os.path.exists(output_path))\n",
    "\n",
    "    def test_job_data_profiler(self):\n",
    "        profiler = JobDataProfiler(df)\n",
    "        profiler.detect_column_types()\n",
    "        self.assertGreaterEqual(len(profiler.categorical_cols + profiler.numerical_cols + profiler.date_cols), 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestJobKPI(unittest.TestCase):\n",
    "    def setUp(self):\n",
    "        transformer = JobDataTransformer(df)\n",
    "        self.transformed_df = transformer.transform()\n",
    "        self.kpi = JobKPI(self.transformed_df)\n",
    "\n",
    "    def test_jobs_per_category(self):\n",
    "        result = self.kpi.jobs_per_category()\n",
    "        self.assertIn(\"jobCategory\", result.columns)\n",
    "        self.assertIn(\"count\", result.columns)\n",
    "\n",
    "    def test_salary_distribution(self):\n",
    "        result = self.kpi.salary_distribution()\n",
    "        self.assertIn(\"jobCategory\", result.columns)\n",
    "        self.assertIn(\"avgSalary\", result.columns)\n",
    "\n",
    "    def test_degree_vs_salary(self):\n",
    "        result = self.kpi.degree_vs_salary()\n",
    "        self.assertIn(\"degreeLevel\", result.columns)\n",
    "        self.assertIn(\"avgSalary\", result.columns)\n",
    "\n",
    "    def test_highest_salary_per_agency(self):\n",
    "        result = self.kpi.highest_salary_per_agency()\n",
    "        self.assertIn(\"agency\", result.columns)\n",
    "        self.assertIn(\"maxSalary\", result.columns)\n",
    "\n",
    "    def test_avg_salary_last_2_years(self):\n",
    "        result = self.kpi.avg_salary_last_2_years()\n",
    "        self.assertIn(\"agency\", result.columns)\n",
    "        self.assertIn(\"avgSalary\", result.columns)\n",
    "\n",
    "    def test_highest_paid_skills(self):\n",
    "        result = self.kpi.highest_paid_skills()\n",
    "        self.assertIn(\"skill\", result.columns)\n",
    "        self.assertIn(\"avgSalary\", result.columns)\n",
    "        self.assertIn(\"skillCount\", result.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:JobDataExtractor:Raw data loaded successfully.\n",
      ".INFO:JobDataExtractor:Data transformed successfully.\n",
      "INFO:JobDataExtractor:Refined data written to /dataset/output/testdata\n",
      ".."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column Type Classification:\n",
      "Categorical Columns: ['agency', 'businessTitle', 'civilServiceTitle', 'jobCategory', 'postingDate', 'minimumQualRequirements', 'preferredSkills']\n",
      "Numerical Columns: ['salaryRangeFrom', 'salaryRangeTo']\n",
      "Date Columns: []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:JobDataExtractor:Data transformed successfully.\n",
      ".INFO:JobDataExtractor:Data transformed successfully.\n",
      "...\n",
      "----------------------------------------------------------------------\n",
      "Ran 7 tests in 23.721s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.runner.TextTestResult run=7 errors=0 failures=0>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unittest.TextTestRunner().run(unittest.defaultTestLoader.loadTestsFromTestCase(TestJobPipelineComponents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
